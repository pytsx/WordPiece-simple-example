{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "36a1b08c",
   "metadata": {},
   "source": [
    "#### WordPiece\n",
    "#### \n",
    "    semelhante ao BPE\n",
    "    ao invés da frequência, escolhe juntar o par de símbolos x1 e x2 que maximiza a seguinte probabilidade no conjunto de treinamento:  \n",
    "#### \n",
    "(P(x2 | x1)) / P(x2)\n",
    "#### \n",
    "##### lê-se: qual a probabilidade do segundo símbolo dado o primeiro dividido pela probabilidade a priori do segundo símbolo "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c97420e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import WordPiece\n",
    "from tokenizers.trainers import WordPieceTrainer \n",
    "from tokenizers.pre_tokenizers import Whitespace\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "83bc0c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [f\"wikitext-103-raw/wiki.{split}.raw\" for split in ['test', 'train', 'valid']]\n",
    "tokenizerWP = Tokenizer(WordPiece(unk_token=\"[UNK]\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89bd65b5",
   "metadata": {},
   "source": [
    "##### Iniciando o módulo de treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b6d8561e",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainerWP = WordPieceTrainer(special_tokens=[\"[UNK]\",\"[CLS]\",\"[SEP]\",\"[PAD]\",\"[MASK]\"],\n",
    "                           vocab_size=30000,\n",
    "                            min_frequency=0,\n",
    "                            continuing_subword_prefix='##')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ffab330",
   "metadata": {},
   "source": [
    "##### Iniciando pré-tokenizador "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "33fd23a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizerWP.pre_tokenizer = Whitespace()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5ea209e",
   "metadata": {},
   "source": [
    "##### Treinando o Tokenizador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3fa4c714",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizerWP.train(files, trainerWP)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c77c50",
   "metadata": {},
   "source": [
    "##### Salvando o Tokenizar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "432919fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizerWP.save('tokenizerWP-wiki.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a57f332f",
   "metadata": {},
   "source": [
    "##### Tokenizando o texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3e225d3f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['No',\n",
       " 'in',\n",
       " '##í',\n",
       " '##ci',\n",
       " '##o',\n",
       " 'de',\n",
       " 'ju',\n",
       " '##l',\n",
       " '##ho',\n",
       " ',',\n",
       " 'em',\n",
       " 'u',\n",
       " '##m',\n",
       " 'cal',\n",
       " '##or',\n",
       " 'su',\n",
       " '##f',\n",
       " '##oca',\n",
       " '##nt',\n",
       " '##e',\n",
       " ',',\n",
       " 'à',\n",
       " '##s',\n",
       " 'qu',\n",
       " '##at',\n",
       " '##ro',\n",
       " 'hor',\n",
       " '##as',\n",
       " 'da',\n",
       " 'tar',\n",
       " '##de',\n",
       " ',',\n",
       " 'u',\n",
       " '##m',\n",
       " 'j',\n",
       " '##ove',\n",
       " '##m',\n",
       " 'sa',\n",
       " '##iu',\n",
       " 'do',\n",
       " 'quar',\n",
       " '##ti',\n",
       " '##n',\n",
       " '##ho',\n",
       " 'al',\n",
       " '##ug',\n",
       " '##ado',\n",
       " 'de',\n",
       " 'u',\n",
       " '##ma',\n",
       " 'cas',\n",
       " '##a',\n",
       " 'de',\n",
       " 'do',\n",
       " '##is',\n",
       " 'and',\n",
       " '##ares',\n",
       " 'no',\n",
       " 'bec',\n",
       " '##o',\n",
       " 'de',\n",
       " 'S',\n",
       " '.',\n",
       " 'e',\n",
       " 'cam',\n",
       " '##in',\n",
       " '##hou',\n",
       " 'lent',\n",
       " '##ament',\n",
       " '##e',\n",
       " ',',\n",
       " 'com',\n",
       " '##o',\n",
       " 'se',\n",
       " 'est',\n",
       " '##ives',\n",
       " '##se',\n",
       " 'hes',\n",
       " '##itan',\n",
       " '##do',\n",
       " ',',\n",
       " 'em',\n",
       " 'dire',\n",
       " '##ç',\n",
       " '##ão',\n",
       " 'à',\n",
       " 'po',\n",
       " '##nt',\n",
       " '##e',\n",
       " 'de',\n",
       " 'K',\n",
       " '.',\n",
       " 'Ele',\n",
       " 'era',\n",
       " 'u',\n",
       " '##m',\n",
       " 'j',\n",
       " '##ove',\n",
       " '##m',\n",
       " 'de',\n",
       " 'cer',\n",
       " '##ca',\n",
       " 'de',\n",
       " 'vin',\n",
       " '##te',\n",
       " 'e',\n",
       " 'c',\n",
       " '##inc',\n",
       " '##o',\n",
       " 'an',\n",
       " '##os',\n",
       " ',',\n",
       " 'mag',\n",
       " '##ro',\n",
       " 'e',\n",
       " 'ba',\n",
       " '##ix',\n",
       " '##o',\n",
       " ',',\n",
       " 'de',\n",
       " 'cab',\n",
       " '##el',\n",
       " '##os',\n",
       " 'cast',\n",
       " '##an',\n",
       " '##ho',\n",
       " '##s',\n",
       " ',',\n",
       " 'ro',\n",
       " '##st',\n",
       " '##o',\n",
       " 'p',\n",
       " '##ál',\n",
       " '##ido',\n",
       " 'e',\n",
       " 'que',\n",
       " 'par',\n",
       " '##ec',\n",
       " '##ia',\n",
       " 'do',\n",
       " '##ente',\n",
       " '.',\n",
       " 'Ves',\n",
       " '##ti',\n",
       " '##a',\n",
       " 'u',\n",
       " '##ma',\n",
       " 'j',\n",
       " '##aque',\n",
       " '##ta',\n",
       " 'de',\n",
       " 'ver',\n",
       " '##ão',\n",
       " 'e',\n",
       " 'u',\n",
       " '##m',\n",
       " 'bon',\n",
       " '##é',\n",
       " 'de',\n",
       " 'pal',\n",
       " '##ha',\n",
       " 'vel',\n",
       " '##ho',\n",
       " ',',\n",
       " 'e',\n",
       " 'car',\n",
       " '##reg',\n",
       " '##ava',\n",
       " 'u',\n",
       " '##ma',\n",
       " 'mo',\n",
       " '##chi',\n",
       " '##la',\n",
       " 'grand',\n",
       " '##e',\n",
       " 'nas',\n",
       " 'cost',\n",
       " '##as',\n",
       " '.']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputWP = tokenizerWP.encode(\"No início de julho, em um calor sufocante, às quatro horas da tarde, um jovem saiu do quartinho alugado de uma casa de dois andares no beco de S. e caminhou lentamente, como se estivesse hesitando, em direção à ponte de K. Ele era um jovem de cerca de vinte e cinco anos, magro e baixo, de cabelos castanhos, rosto pálido e que parecia doente. Vestia uma jaqueta de verão e um boné de palha velho, e carregava uma mochila grande nas costas.\")\n",
    "outputWP.tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a01e66e2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
